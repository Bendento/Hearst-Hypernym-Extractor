{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"# Import libraries required\nimport re\nimport nltk\nfrom nltk.tag.perceptron import PerceptronTagger"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"# Noun phrase chunking\nchunk_patterns=r'''NP:{<DT>?<JJ.*>*<NN.*>+}\n                      {<NN.*>+}\n                '''\n#chunk parser\nnounphrase_chunker=nltk.RegexpParser(chunk_patterns)\n# Hearst Patterns\nhearst_patterns=[ (\n                '(NP_\\\\w+ (, )?such as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n                'first'\n            ),\n            (\n                '(such NP_\\\\w+ (, )?as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n                'first'\n            ),\n            (\n                '((NP_\\\\w+ ?(, )?)+(and |or )?other NP_\\\\w+)',\n                'last'\n            ),\n            (\n                '(NP_\\\\w+ (, )?include (NP_\\\\w+ ?(, )?(and |or )?)+)',\n                'first'\n            ),\n            (\n                '(NP_\\\\w+ (, )?especially (NP_\\\\w+ ?(, )?(and |or )?)+)',\n                'first'\n            ),\n            (\n                    '((NP_\\\\w+ ?(, )?)+(and |or )?any other NP_\\\\w+)',\n                    'last'\n                ),\n                (\n                    '((NP_\\\\w+ ?(, )?)+(and |or )?some other NP_\\\\w+)',\n                    'last'\n                ),\n                (\n                    '((NP_\\\\w+ ?(, )?)+(and |or )?be a NP_\\\\w+)',\n                    'last'\n                ),\n                (\n                    '(NP_\\\\w+ (, )?like (NP_\\\\w+ ? (, )?(and |or )?)+)',\n                    'first'\n                ),\n                (\n                    'such (NP_\\\\w+ (, )?as (NP_\\\\w+ ? (, )?(and |or )?)+)',\n                    'first'\n                ),\n                (\n                    '((NP_\\\\w+ ?(, )?)+(and |or )?like other NP_\\\\w+)',\n                    'last'\n                ),\n                (\n                    '((NP_\\\\w+ ?(, )?)+(and |or )?one of the NP_\\\\w+)',\n                    'last'\n                ),\n                (\n                    '((NP_\\\\w+ ?(, )?)+(and |or )?one of these NP_\\\\w+)',\n                    'last'\n                ),\n                (\n                    '((NP_\\\\w+ ?(, )?)+(and |or )?one of those NP_\\\\w+)',\n                    'last'\n                ),\n                (\n                    'example of (NP_\\\\w+ (, )?be (NP_\\\\w+ ? '\n                    '(, )?(and |or )?)+)',\n                    'first'\n                ),\n                (\n                    '((NP_\\\\w+ ?(, )?)+(and |or )?be example of NP_\\\\w+)',\n                    'last'\n                ),\n                (\n                    '(NP_\\\\w+ (, )?for example (, )?'\n                    '(NP_\\\\w+ ?(, )?(and |or )?)+)',\n                    'first'\n                ),\n                (\n                    '((NP_\\\\w+ ?(, )?)+(and |or )?which be call NP_\\\\w+)',\n                    'last'\n                ),\n                (\n                    '((NP_\\\\w+ ?(, )?)+(and |or )?which be name NP_\\\\w+)',\n                    'last'\n                ),\n                (\n                    '(NP_\\\\w+ (, )?mainly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n                    'first'\n                ),\n                (\n                    '(NP_\\\\w+ (, )?mostly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n                    'first'\n                ),\n                (\n                    '(NP_\\\\w+ (, )?notably (NP_\\\\w+ ? (, )?(and |or )?)+)',\n                    'first'\n                ),\n                (\n                    '(NP_\\\\w+ (, )?particularly (NP_\\\\w+ ? '\n                    '(, )?(and |or )?)+)',\n                    'first'\n                ),\n                (\n                    '(NP_\\\\w+ (, )?principally (NP_\\\\w+ ? (, )?(and |or )?)+)',\n                    'first'\n                ),\n                (\n                    '(NP_\\\\w+ (, )?in particular (NP_\\\\w+ ? '\n                    '(, )?(and |or )?)+)',\n                    'first'\n                ),\n                (\n                    '(NP_\\\\w+ (, )?except (NP_\\\\w+ ? (, )?(and |or )?)+)',\n                    'first'\n                ),\n                (\n                    '(NP_\\\\w+ (, )?other than (NP_\\\\w+ ? (, )?(and |or )?)+)',\n                    'first'\n                ),\n                (\n                    '(NP_\\\\w+ (, )?e.g. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n                    'first'\n                ),\n                (\n                    '(NP_\\\\w+ \\\\( (e.g.|i.e.) (, )?(NP_\\\\w+ ? (, )?(and |or )?)+'\n                    '(\\\\. )?\\\\))',\n                    'first'\n                ),\n                (\n                    '(NP_\\\\w+ (, )?i.e. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n                    'first'\n                ),\n                (\n                    '((NP_\\\\w+ ?(, )?)+(and|or)? a kind of NP_\\\\w+)',\n                    'last'\n                ),\n                (\n                    '((NP_\\\\w+ ?(, )?)+(and|or)? kind of NP_\\\\w+)',\n                    'last'\n                ),\n                (\n                    '((NP_\\\\w+ ?(, )?)+(and|or)? form of NP_\\\\w+)',\n                    'last'\n                ),\n                (\n                    '((NP_\\\\w+ ?(, )?)+(and |or )?which look like NP_\\\\w+)',\n                    'last'\n                ),\n                (\n                    '((NP_\\\\w+ ?(, )?)+(and |or )?which sound like NP_\\\\w+)',\n                    'last'\n                ),\n                (\n                    '(NP_\\\\w+ (, )?which be similar to (NP_\\\\w+ ? '\n                    '(, )?(and |or )?)+)',\n                    'first'\n                ),\n                (\n                    '(NP_\\\\w+ (, )?example of this be (NP_\\\\w+ ? '\n                    '(, )?(and |or )?)+)',\n                    'first'\n                ),\n                (\n                    '(NP_\\\\w+ (, )?type (NP_\\\\w+ ? (, )?(and |or )?)+)',\n                    'first'\n                ),\n                (\n                    '((NP_\\\\w+ ?(, )?)+(and |or )? NP_\\\\w+ type)',\n                    'last'\n                ),\n                (\n                    '(NP_\\\\w+ (, )?whether (NP_\\\\w+ ? (, )?(and |or )?)+)',\n                    'first'\n                ),\n                (\n                    '(compare (NP_\\\\w+ ?(, )?)+(and |or )?with NP_\\\\w+)',\n                    'last'\n                ),\n                (\n                    '(NP_\\\\w+ (, )?compare to (NP_\\\\w+ ? (, )?(and |or )?)+)',\n                    'first'\n                ),\n                (\n                    '(NP_\\\\w+ (, )?among -PRON- (NP_\\\\w+ ? '\n                    '(, )?(and |or )?)+)',\n                    'first'\n                ),\n                (\n                    '((NP_\\\\w+ ?(, )?)+(and |or )?as NP_\\\\w+)',\n                    'last'\n                ),\n                (\n                    '(NP_\\\\w+ (, )? (NP_\\\\w+ ? (, )?(and |or )?)+ '\n                    'for instance)',\n                    'first'\n                ),\n                (\n                    '((NP_\\\\w+ ?(, )?)+(and|or)? sort of NP_\\\\w+)',\n                    'last'\n                ),\n                (\n                    '(NP_\\\\w+ (, )?which may include (NP_\\\\w+ '\n                    '?(, )?(and |or )?)+)',\n                    'first'\n                ),]"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"# Part of Speech tagging\npos_tagger=PerceptronTagger()"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"# Text Preprocessing\ndef prepare(raw_text):\n    sentences=nltk.sent_tokenize(raw_text.strip())\n    sentences=[nltk.word_tokenize(sent) for sent in sentences]\n    sentences=[pos_tagger.tag(sent) for sent in sentences]\n    return sentences"},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":"def chunk(rawtext):\n    sentences=prepare(rawtext.strip())\n    all_chunks=[]\n    for sentence in sentences:\n        chunks=nounphrase_chunker.parse(sentence)\n        all_chunks.append(prepare_chunks(chunks))\n    # If we have more than 1 consecutive Noun phrase, we merge into one single noun phrase\n    all_sentences=[]\n    for raw_sentence in all_chunks:\n        sentence = re.sub(r\"(NP_\\w+ NP_\\w+)+\",\n                              lambda m: m.expand(r'\\1').replace(\" NP_\", \"_\"),\n                              raw_sentence)\n        all_sentences.append(sentence)\n\n    return all_sentences"},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":"# Method to replace occurrences in text that are noun phrases and make them start with NP_<token> or just keep it as is\ndef prepare_chunks(chunks):\n    terms=[]\n    for chunk in chunks:\n        label=None\n        try:\n            label=chunk.label()\n        except:\n            pass\n        if label is None:\n            token=chunk[0]\n            terms.append(token)\n        else:\n            np='NP_'+'_'.join([a[0] for a in chunk])\n            terms.append(np)\n    return ' '.join(terms)"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"# Test preprocessing\nex_corpus='I like music like rock and metal. I think countries like Iceland and Norway are beautiful. It is a good day to die.'\nsentences=nltk.sent_tokenize(ex_corpus)\nsentences=[nltk.word_tokenize(sent) for sent in sentences]\nsentences=[pos_tagger.tag(sent) for sent in sentences]\nchunk=nounphrase_chunker.parse(sentences[0])\nnp_tagged_sentence=prepare_chunks(chunk)"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"# for chunko in chunk:\n#     print(chunko[1])\n# for (hearst_pattern,parser) in hearst_patterns:\n#     print(hearst_pattern)\nmatches=re.search(list(hearst_patterns[8])[0],np_tagged_sentence)"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":"['NP_music', 'NP_rock', 'NP_metal']"},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":"match_str=matches.group(0)\n[a for a in match_str.split() if a.startswith('NP_')]"},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":"def remove_np_term(term):\n    return term.replace('NP_','').replace('_',' ')"},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":"def find_hyponyms(rawtext):\n    hypo_hypernyms=[]\n    np_tagged_sentences=chunk(rawtext)\n\n    for sentence in np_tagged_sentences:\n        for (hearst_pattern,parser) in hearst_patterns:\n            matches=re.search(hearst_pattern,sentence)\n            if matches:\n                match_str=matches.group(0)\n                nps=[a for a in match_str.split() if a.startswith('NP_')]\n\n                if parser=='first':\n                    hypernym=nps[0]\n                    hyponyms=nps[1:]\n                else:\n                    hypernym=nps[-1]\n                    hyponyms=nps[:-1]\n                for i in range(len(hyponyms)):\n                    hypo_hypernyms.append((remove_np_term(hyponyms[i]),remove_np_term(hypernym)))\n    return hypo_hypernyms"},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":"data=prepare(ex_corpus)\nall_chunks=[]"},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":"[('rock', 'music'),\n ('metal', 'music'),\n ('Iceland', 'countries'),\n ('Norway', 'countries')]"},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":"find_hyponyms(ex_corpus)"},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":"# Apply Hearst Patterns to Wikipedia Files\ndef extractHearstWikipedia(input):\n    extractions=[]\n    lines_processed=0\n\n    with open(input,'r') as f:\n        for  line in f:\n            lines_processed+=1\n            line=line.strip()\n            if not line:\n                continue\n            line_split=line.split('\\t')\n            sentence,lemma_sent=line_split[0].strip(),line_split[1].strip()\n            hypo_hyper_pairs=find_hyponyms(sentence)\n            extractions.append(hypo_hyper_pairs)\n\n            if lines_processed%1000==0:\n                print('Lines Processed: {}'.format(lines_processed))\n\n    return extractions\n"},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Lines Processed: 1000\nLines Processed: 2000\nLines Processed: 3000\nLines Processed: 4000\nLines Processed: 5000\nLines Processed: 6000\nLines Processed: 7000\nLines Processed: 8000\nLines Processed: 9000\nLines Processed: 10000\nLines Processed: 11000\nLines Processed: 12000\nLines Processed: 13000\nLines Processed: 14000\nLines Processed: 15000\nLines Processed: 16000\nLines Processed: 17000\nLines Processed: 18000\nLines Processed: 19000\nLines Processed: 20000\nLines Processed: 21000\nLines Processed: 22000\nLines Processed: 23000\nLines Processed: 24000\nLines Processed: 25000\nLines Processed: 26000\nLines Processed: 27000\nLines Processed: 28000\nLines Processed: 29000\nLines Processed: 30000\nLines Processed: 31000\nLines Processed: 32000\nLines Processed: 33000\nLines Processed: 34000\nLines Processed: 35000\nLines Processed: 36000\nLines Processed: 37000\nLines Processed: 38000\nLines Processed: 39000\nLines Processed: 40000\nLines Processed: 41000\nLines Processed: 42000\nLines Processed: 43000\nLines Processed: 44000\nLines Processed: 45000\nLines Processed: 46000\nLines Processed: 47000\nLines Processed: 48000\nLines Processed: 49000\nLines Processed: 50000\nLines Processed: 51000\nLines Processed: 52000\nLines Processed: 53000\nLines Processed: 54000\nLines Processed: 55000\nLines Processed: 56000\nLines Processed: 57000\nLines Processed: 58000\nLines Processed: 59000\nLines Processed: 60000\nLines Processed: 61000\nLines Processed: 62000\nLines Processed: 63000\nLines Processed: 64000\nLines Processed: 65000\nLines Processed: 66000\nLines Processed: 67000\nLines Processed: 68000\nLines Processed: 69000\nLines Processed: 70000\nLines Processed: 71000\nLines Processed: 72000\nLines Processed: 73000\nLines Processed: 74000\nLines Processed: 75000\nLines Processed: 76000\nLines Processed: 77000\nLines Processed: 78000\nLines Processed: 79000\nLines Processed: 80000\nLines Processed: 81000\nLines Processed: 82000\nLines Processed: 83000\nLines Processed: 84000\nLines Processed: 85000\nLines Processed: 86000\nLines Processed: 87000\nLines Processed: 88000\nLines Processed: 89000\nLines Processed: 90000\nLines Processed: 91000\nLines Processed: 92000\nLines Processed: 93000\nLines Processed: 94000\nLines Processed: 95000\nLines Processed: 96000\nLines Processed: 97000\nLines Processed: 98000\nLines Processed: 99000\nLines Processed: 100000\nLines Processed: 101000\nLines Processed: 102000\nLines Processed: 103000\nLines Processed: 104000\nLines Processed: 105000\nLines Processed: 106000\nLines Processed: 107000\nLines Processed: 108000\nLines Processed: 109000\nLines Processed: 110000\nLines Processed: 111000\nLines Processed: 112000\nLines Processed: 113000\nLines Processed: 114000\nLines Processed: 115000\nLines Processed: 116000\nLines Processed: 117000\nLines Processed: 118000\nLines Processed: 119000\nLines Processed: 120000\nLines Processed: 121000\nLines Processed: 122000\nLines Processed: 123000\nLines Processed: 124000\nLines Processed: 125000\nLines Processed: 126000\nLines Processed: 127000\nLines Processed: 128000\nLines Processed: 129000\nLines Processed: 130000\nLines Processed: 131000\nLines Processed: 132000\nLines Processed: 133000\nLines Processed: 134000\nLines Processed: 135000\nLines Processed: 136000\nLines Processed: 137000\nLines Processed: 138000\nLines Processed: 139000\nLines Processed: 140000\nLines Processed: 141000\nLines Processed: 142000\nLines Processed: 143000\nLines Processed: 144000\nLines Processed: 145000\nLines Processed: 146000\nLines Processed: 147000\nLines Processed: 148000\nLines Processed: 149000\nLines Processed: 150000\nLines Processed: 151000\nLines Processed: 152000\nLines Processed: 153000\nLines Processed: 154000\nLines Processed: 155000\nLines Processed: 156000\nLines Processed: 157000\nLines Processed: 158000\nLines Processed: 159000\nLines Processed: 160000\nLines Processed: 161000\nLines Processed: 162000\nLines Processed: 163000\nLines Processed: 164000\nLines Processed: 165000\nLines Processed: 166000\nLines Processed: 167000\nLines Processed: 168000\nLines Processed: 169000\nLines Processed: 170000\nLines Processed: 171000\nLines Processed: 172000\nLines Processed: 173000\nLines Processed: 174000\nLines Processed: 175000\nLines Processed: 176000\nLines Processed: 177000\nLines Processed: 178000\nLines Processed: 179000\nLines Processed: 180000\nLines Processed: 181000\nLines Processed: 182000\nLines Processed: 183000\nLines Processed: 184000\nLines Processed: 185000\nLines Processed: 186000\nLines Processed: 187000\nLines Processed: 188000\nLines Processed: 189000\nLines Processed: 190000\nLines Processed: 191000\nLines Processed: 192000\nLines Processed: 193000\nLines Processed: 194000\nLines Processed: 195000\nLines Processed: 196000\nLines Processed: 197000\nLines Processed: 198000\nLines Processed: 199000\nLines Processed: 200000\nLines Processed: 201000\nLines Processed: 202000\nLines Processed: 203000\nLines Processed: 204000\nLines Processed: 205000\nLines Processed: 206000\nLines Processed: 207000\nLines Processed: 208000\nLines Processed: 209000\nLines Processed: 210000\nLines Processed: 211000\nLines Processed: 212000\nLines Processed: 213000\nLines Processed: 214000\nLines Processed: 215000\nLines Processed: 216000\nLines Processed: 217000\nLines Processed: 218000\nLines Processed: 219000\nLines Processed: 220000\nLines Processed: 221000\nLines Processed: 222000\nLines Processed: 223000\nLines Processed: 224000\nLines Processed: 225000\nLines Processed: 226000\nLines Processed: 227000\nLines Processed: 228000\nLines Processed: 229000\nLines Processed: 230000\nLines Processed: 231000\nLines Processed: 232000\nLines Processed: 233000\nLines Processed: 234000\nLines Processed: 235000\nLines Processed: 236000\nLines Processed: 237000\nLines Processed: 238000\nLines Processed: 239000\nLines Processed: 240000\nLines Processed: 241000\nLines Processed: 242000\nLines Processed: 243000\nLines Processed: 244000\nLines Processed: 245000\nLines Processed: 246000\nLines Processed: 247000\nLines Processed: 248000\nLines Processed: 249000\nLines Processed: 250000\nLines Processed: 251000\nLines Processed: 252000\nLines Processed: 253000\nLines Processed: 254000\nLines Processed: 255000\nLines Processed: 256000\nLines Processed: 257000\nLines Processed: 258000\nLines Processed: 259000\nLines Processed: 260000\nLines Processed: 261000\nLines Processed: 262000\nLines Processed: 263000\nLines Processed: 264000\nLines Processed: 265000\nLines Processed: 266000\nLines Processed: 267000\nLines Processed: 268000\nLines Processed: 269000\nLines Processed: 270000\nLines Processed: 271000\nLines Processed: 272000\nLines Processed: 273000\nLines Processed: 274000\nLines Processed: 275000\nLines Processed: 276000\nLines Processed: 277000\nLines Processed: 278000\nLines Processed: 279000\nLines Processed: 280000\nLines Processed: 281000\nLines Processed: 282000\nLines Processed: 283000\nLines Processed: 284000\nLines Processed: 285000\nLines Processed: 286000\nLines Processed: 287000\nLines Processed: 288000\n"}],"source":"hypo_hyper_pairs=extractHearstWikipedia('/home/shaurya/Documents/lexicalinference/wikipedia_sentences.txt')"},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":"# Write Hypernyms to a file\nwith open('/home/shaurya/Documents/lexicalinference/wikipedia_hypernyms.txt','w') as f:\n    for (hypo,hyper) in hypo_hypernyms:\n        try:\n            f.write(hypo+'\\t'+hyper+'\\n')\n        except ValueError:\n            pass\n"},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"data":{"text/plain":"[[],\n [],\n [('the guitar serve', 'companions')],\n [],\n [('troops', 'a section')],\n [],\n [],\n [],\n [],\n []]"},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":"hypo_hyper_pairs[:10]"},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":"hypo_hypernyms=[x for x in hypo_hyper_pairs if x!=[]]\n"},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":"import itertools\nhypo_hypernyms=list(itertools.chain.from_iterable(hypo_hypernyms))"},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":"# Convert the Extractions from Wikipedia into a dictionary\nhyperhypo={}\nwith open('/home/shaurya/Documents/lexicalinference/wikipedia_hypernyms.txt','r') as f:\n    text=f.read().strip().split('\\n')\n    for line in text:\n        hyponym,hypernym=line.split('\\t')\n        hyponym,hypernym=hyponym.lower(),hypernym.lower()\n        hyperhypo[hypernym]=hyponym\n        "},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# "}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}